{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ADMxYciXidw",
        "outputId": "b65b64dd-1c6f-4e89-c2d2-9d9059904894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=e208d670c8412500c634abe8f5320875bb799cdc77bfe64d48790da78e5f6d5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "KywgC1lJXx7o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"IR-Inverted-index\").config(\"spark.executor.memory\", \"1g\").config(\"spark.executor.cores\", 2).getOrCreate()"
      ],
      "metadata": {
        "id": "9UUxHZklX0Oy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "qJQ6nxJCYYw6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"/content/data2.zip\"\n",
        "extract_folder = \"/content/extracted_files/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)"
      ],
      "metadata": {
        "id": "OSqrzRLmYAIE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_rdd = spark.sparkContext.wholeTextFiles(\"/content/extracted_files/data/devdata/*.txt\")"
      ],
      "metadata": {
        "id": "vE4q6Rr9YSIn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df = bigram_rdd.toDF()"
      ],
      "metadata": {
        "id": "OVEtHIarYeo4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFCYCsdqYhu2",
        "outputId": "ee809c59-3b15-4002-b052-d47dd0375ca2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                  _1|                  _2|\n",
            "+--------------------+--------------------+\n",
            "|file:/content/ext...|5722018101\\tEvery...|\n",
            "|file:/content/ext...|5722018235\\tThen ...|\n",
            "|file:/content/ext...|5722018301\\tKevin...|\n",
            "|file:/content/ext...|5722018496\\t\"It i...|\n",
            "|file:/content/ext...|5722018508\\tQuali...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace, lower,expr,explode, split,concat"
      ],
      "metadata": {
        "id": "U1NNFMARYoVK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df = bigram_df.withColumn(\"text_cleaned\", regexp_replace(\"_2\", \"(?<=\\t)\", \"\"))"
      ],
      "metadata": {
        "id": "6vXqVTxYYjxs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df = bigram_df.withColumn(\"doc_id\", expr(\"split(text_cleaned, '\\t')[0]\"))\n",
        "\n",
        "bigram_df = bigram_df.withColumn(\"final_text\", expr(\"lower(regexp_replace(trim(substring(text_cleaned, length(split(text_cleaned, '\\t')[0])+2)), '[^a-zA-Z\\\\s]', ' '))\"))"
      ],
      "metadata": {
        "id": "OYSDaDXo7YeK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbmejOVxZttS",
        "outputId": "2f265ac4-15ed-47c3-c771-9605a5e555d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+----------+--------------------+\n",
            "|                  _1|                  _2|        text_cleaned|    doc_id|          final_text|\n",
            "+--------------------+--------------------+--------------------+----------+--------------------+\n",
            "|file:/content/ext...|5722018101\\tEvery...|5722018101\\tEvery...|5722018101|every person has ...|\n",
            "|file:/content/ext...|5722018235\\tThen ...|5722018235\\tThen ...|5722018235|then jon falls fo...|\n",
            "|file:/content/ext...|5722018301\\tKevin...|5722018301\\tKevin...|5722018301|kevin  take a loo...|\n",
            "|file:/content/ext...|5722018496\\t\"It i...|5722018496\\t\"It i...|5722018496| it is an area wi...|\n",
            "|file:/content/ext...|5722018508\\tQuali...|5722018508\\tQuali...|5722018508|qualified candida...|\n",
            "+--------------------+--------------------+--------------------+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, lag, concat_ws, collect_list\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "nRDv22fMaN_N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_df = bigram_df.select(\"doc_id\", F.explode(F.split(F.col(\"final_text\"), \" \")).alias(\"word\"))\n",
        "words_df = words_df.filter(F.col(\"word\") != \"\")\n",
        "\n",
        "window = Window.partitionBy(\"doc_id\").orderBy(\"doc_id\")\n",
        "words_df = words_df.withColumn(\"prev_word\", F.lag(\"word\").over(window))\n",
        "\n",
        "words_df = words_df.filter(F.col(\"prev_word\").isNotNull())"
      ],
      "metadata": {
        "id": "aylzlT26aa8_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_index = (\n",
        "    words_df\n",
        "    .groupby(\"prev_word\", \"word\", \"doc_id\")\n",
        "    .agg(F.count(\"doc_id\").alias(\"doc_id_count\"))\n",
        ")\n",
        "\n",
        "bigram_index_formatted = bigram_index.groupBy(\"prev_word\", \"word\").agg(F.collect_list(F.concat_ws(':', F.col(\"doc_id\"), F.col(\"doc_id_count\"))).alias(\"doc_ids\"))\n",
        "\n",
        "output_bigram = bigram_index_formatted.withColumn(\"bigram_data\", F.concat_ws(' ', F.col(\"prev_word\"), F.col(\"word\"), F.expr(\"concat_ws(' ', doc_ids)\")))\n",
        "\n",
        "output_bigram.select(\"bigram_data\").repartition(1).write.mode(\"overwrite\").text(\"output_bigram\")"
      ],
      "metadata": {
        "id": "PhLoaLPOafg4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = output_bigram.select(\"bigram_data\")"
      ],
      "metadata": {
        "id": "J8VKfUaias_y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "final_result = output_bigram.select(\"bigram_data\")\n",
        "\n",
        "phrases = [\"computer science\", \"information retrieval\", \"power politics\", \"los angeles\", \"bruce willis\"]\n",
        "\n",
        "condition = col(\"bigram_data\").rlike(\"|\".join(phrases))\n",
        "\n",
        "filtered_df = final_result.filter(condition)\n",
        "\n",
        "rdd = filtered_df.rdd.map(lambda row: row[0])\n",
        "\n",
        "output_path = \"selected_bigram_index.txt\"\n",
        "\n",
        "rdd.coalesce(1).saveAsTextFile(output_path)"
      ],
      "metadata": {
        "id": "9rZnjVwMawgx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsF1bpA_bV4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}